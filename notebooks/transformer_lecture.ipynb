{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMUKDJXULFBa"
      },
      "source": [
        "# Bigram LM as based for a Transformer LM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BVHLF48W-dl-"
      },
      "outputs": [],
      "source": [
        "with open(\"../input/fortune-messages.txt\") as f:\n",
        "  text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dNkbuzZi5mJT"
      },
      "outputs": [],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5gyl6dw7cHq",
        "outputId": "1c6f2d92-4ffe-4521-9129-18bd63b86e23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[46, 43, 50, 50, 53]\n",
            "hello\n"
          ]
        }
      ],
      "source": [
        "char_to_token = { c:i for i, c in enumerate(chars) }\n",
        "token_to_char = { i:c for i, c in enumerate(chars) }\n",
        "encode = lambda s: [ char_to_token[c] for c in s ]\n",
        "decode = lambda l: \"\".join([ token_to_char[t] for t in l])\n",
        "\n",
        "print(encode(\"hello\"))\n",
        "print(decode(encode(\"hello\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bdZ9FniUB4G",
        "outputId": "66e57d0a-fd65-4e9f-9748-ba40f7e59261"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([13083])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "print(data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijWMS5dNZdNF",
        "outputId": "771ba116-b85c-40e7-b7fc-3653aa070543"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For this chunk (x) tensor([37, 47, 58, 46,  1, 47, 52, 58])\n",
            "When the input is tensor([37]), we expect 47\n",
            "When the input is tensor([37, 47]), we expect 58\n",
            "When the input is tensor([37, 47, 58]), we expect 46\n",
            "When the input is tensor([37, 47, 58, 46]), we expect 1\n",
            "When the input is tensor([37, 47, 58, 46,  1]), we expect 47\n",
            "When the input is tensor([37, 47, 58, 46,  1, 47]), we expect 52\n",
            "When the input is tensor([37, 47, 58, 46,  1, 47, 52]), we expect 58\n",
            "When the input is tensor([37, 47, 58, 46,  1, 47, 52, 58]), we expect 43\n"
          ]
        }
      ],
      "source": [
        "# For each x and y, we get block_size training samples\n",
        "block_size = 8\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "print(\"For this chunk (x)\", train_data[:block_size])\n",
        "for i in range(block_size):\n",
        "  context = x[:i+1]\n",
        "  target = y[i]\n",
        "  print(f\"When the input is {context}, we expect {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mifGycUzeepe",
        "outputId": "141bab3e-3141-4c72-84a2-fc67732d9d18"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[tensor([[63,  1, 58, 46, 43,  1, 61, 47],\n",
              "         [39, 42, 51, 47, 56, 43, 42,  9],\n",
              "         [47, 52, 45,  1, 47, 57,  1, 58],\n",
              "         [56, 58,  1, 58, 53,  1, 50, 53]]),\n",
              " tensor([[ 1, 58, 46, 43,  1, 61, 47, 57],\n",
              "         [42, 51, 47, 56, 43, 42,  9,  0],\n",
              "         [52, 45,  1, 47, 57,  1, 58, 53],\n",
              "         [58,  1, 58, 53,  1, 50, 53, 53]])]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_size = 4\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def get_batch(source):\n",
        "  data = train_data if source == \"train\" else val_data\n",
        "  rv = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  inputs = torch.stack([data[r:r+block_size] for r in rv])\n",
        "  outputs = torch.stack([data[r+1:r+block_size+1] for r in rv])\n",
        "  inputs, outputs = inputs.to(device), outputs.to(device)\n",
        "\n",
        "  return [inputs, outputs]\n",
        "\n",
        "xb, yb = get_batch(\"train\")\n",
        "get_batch(\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9KC3TkXcyWdX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as f\n",
        "\n",
        "class BigramLM(nn.Module):\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  # idx is a tensor of shape (B, T): batch size B, sequence length T\n",
        "  def forward(self, idx, targets=None):\n",
        "    # For each token in the input, retrieve its embedding\n",
        "    # Result is shape (B, T, C), where C = vocab_size\n",
        "    # Since this is a Bigram model, the embedding gives the posiblidates of the followin token\n",
        "    logits = self.token_embedding_table(idx)\n",
        "\n",
        "    if targets == None:\n",
        "      return logits, None\n",
        "\n",
        "    B, T, C = logits.shape\n",
        "\n",
        "    # The cross entropy function expects logits and targets\n",
        "    # The logits tensor should have shape (B, C), where B is the total number of samples (e.g., batch size),\n",
        "    # and C is the number of classes (for example, 10 if you're classifying digits 0–9)\n",
        "    # The target should be a tensor of shape (B,), where each element is the class index\n",
        "    # for each corresponding sample in the batch (e.g., a number from 0 to 9 in digit classification)\n",
        "    logits = logits.view(B * T, C)\n",
        "    target = targets.view(B * T)\n",
        "    cost = f.cross_entropy(logits, target)\n",
        "\n",
        "    return logits, cost\n",
        "\n",
        "  def generate(self, idx, new_tokens):\n",
        "    for _ in range(new_tokens):\n",
        "      logits, _ = self(idx)\n",
        "\n",
        "      # We take only the logits from the last time step (T),\n",
        "      # which gives us the predicted probabilities for the next token.\n",
        "      # Selecting the last index from the second dimension gives a tensor of shape (B, C),\n",
        "      # where B is the batch size and C is the vocabulary size.\n",
        "      logits = logits[:, -1, :]\n",
        "\n",
        "      # Apply softmax across the last dimension (C) to convert logits to probabilities\n",
        "      probs = f.softmax(logits, dim=-1)\n",
        "\n",
        "      # Sample the next token from the probability distribution\n",
        "      # The result has shape (B, 1), where each element is the sampled token index\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "      # Append the sampled token to the existing sequence\n",
        "      # The new idx has shape (B, T+1)\n",
        "      idx = torch.cat((idx, idx_next), dim=-1)\n",
        "    return idx\n",
        "\n",
        "model = BigramLM(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iu2rLjFzE4Gf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Text generation before training a Bigram model:  \n",
            "VI.RiV,Ufb?pRA\n",
            "J,u )'oED-sr1JAEa-uYslDPlqNdqKTN;Kppt.duARLg.::D-tmwJ:gcF:gnjL1MgRivo'cMWTcNNuAzF)FT\"B\n",
            "!3;qzrh.TJ2yAvLfiee1f;Mj!a-mTvL(xa:,;:cM.McFdKlqhn31J,H,hCycTS\"hLsJm.M(qcSmlCMn:WpY,E'x(zzNibhFweM\n",
            "-- Text generation before training a Bigram model:  \n",
            "Bensug youatoyoromeaner of ay pl!\n",
            "You sprperel t s.\n",
            "En ub, ayou etour t.\n",
            "RR(3:Do yo me n pa cer ivifre indgillll wmo tutinere wita u cerowilquro w berchexceven yoee ang g r ORke.\n",
            "Thee.\n",
            "Yoknurk, pthene\n",
            "Cost: 2.246943235397339\n"
          ]
        }
      ],
      "source": [
        "batch_size = 30\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "print(f\"-- Text generation before training a Bigram model: \",\n",
        "  decode(model.generate(torch.zeros(1, 1, dtype=torch.long), 200)[0].tolist()))\n",
        "\n",
        "for steps in range(10000):\n",
        "  xb, yb = get_batch(\"train\")\n",
        "\n",
        "  logits, cost = model(xb, yb)\n",
        "\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(f\"-- Text generation before training a Bigram model: \",\n",
        "  decode(model.generate(torch.zeros(1, 1, dtype=torch.long), 200)[0].tolist()))\n",
        "print(f\"Cost: {cost.item()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zDN83XjKwu5"
      },
      "source": [
        "# Matematical trick in seft-attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kj2cJYrHK8XQ",
        "outputId": "c3176eea-e9c2-4316-8498-d4ca0fb16c12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.1419,  0.3039],\n",
            "        [-0.2960,  0.4303],\n",
            "        [-0.3506,  0.3831],\n",
            "        [-0.7844,  0.9591],\n",
            "        [ 1.1926, -0.7318],\n",
            "        [-0.5843,  0.9416],\n",
            "        [-0.5539, -0.5826],\n",
            "        [-0.3922, -0.1083]]) tensor([[-0.1419,  0.3039],\n",
            "        [-0.2189,  0.3671],\n",
            "        [-0.2628,  0.3724],\n",
            "        [-0.3932,  0.5191],\n",
            "        [-0.0761,  0.2689],\n",
            "        [-0.1608,  0.3810],\n",
            "        [-0.2169,  0.2434],\n",
            "        [-0.2388,  0.1994]])\n"
          ]
        }
      ],
      "source": [
        "B,T,C = 4,8,2\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# bow stands for bag of words, is the tensor where we are going to store the avg\n",
        "xbow = torch.zeros((B,T,C))\n",
        "\n",
        "for b in range(B):\n",
        "  for t in range(T):\n",
        "    xprev = x[b,:t+1] #(t + 1, C)\n",
        "\n",
        "    # We generate a new embedding that is the average of all previous ones (including the current one)\n",
        "    # The average is taken along dimension 0 — that is, across rows\n",
        "    xbow[b,t] = torch.mean(xprev, 0)\n",
        "\n",
        "print(x[0], xbow[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kANEryAjZaKz",
        "outputId": "b1c8e34e-9633-4eed-a63e-7cfef35203c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1., 0., 0.],\n",
            "        [1., 1., 0.],\n",
            "        [1., 1., 1.]])\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "tensor([[7., 8.],\n",
            "        [2., 4.],\n",
            "        [9., 5.]])\n",
            "tensor([[7.0000, 8.0000],\n",
            "        [4.5000, 6.0000],\n",
            "        [6.0000, 5.6667]])\n"
          ]
        }
      ],
      "source": [
        "# Because we're using for loops, we're not being efficient.\n",
        "# But we can achieve the same result using matrix multiplication.\n",
        "\n",
        "# Let's use a new matrix wei to compute the average of the previous embeddings\n",
        "wei = torch.tril(torch.ones(3, 3))\n",
        "print(wei)\n",
        "\n",
        "# Configure the wei matrix to compute the average of the previous embeddings across dimension 0\n",
        "# Each row is normalized by the number of ones (i.e., number of elements being averaged)\n",
        "wei = wei / torch.sum(wei, 1, keepdim=True)\n",
        "\n",
        "# Create an example matrix x, where each row is a \"time step\" with an embedding of size 2\n",
        "x = torch.randint(0, 10, (3, 2)).float()\n",
        "\n",
        "print(wei)\n",
        "print(x)\n",
        "print(wei @ x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BWgQAjt-3aEB"
      },
      "outputs": [],
      "source": [
        "# Example of a single head of self attention as a decoder\n",
        "\n",
        "B,T,C = 4,8,2\n",
        "head_size = 16\n",
        "\n",
        "x = torch.randn(B,T,C)\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "# This is self-attention because keys, queries, and values all come from the same input x\n",
        "\n",
        "# Queries: \"What should I pay attention to?\"\n",
        "q = query(x)  # Shape: (B, T, head_size)\n",
        "\n",
        "# Keys: \"What information do I contain?\"\n",
        "k = key(x)    # Shape: (B, T, head_size)\n",
        "\n",
        "# Values: \"What information should actually be passed on?\"\n",
        "# The model needs more flexibility and capacity to learn rich patterns than what raw embeddings provide.\n",
        "v = value(x)  # Shape: (B, T, head_size)\n",
        "\n",
        "# Compute attention scores: dot product between queries and keys\n",
        "# Shape: (B, T, T) — how much each token should attend to every other token\n",
        "wei = q @ k.transpose(-2, -1)  # Transpose swaps last two dims of k to match q\n",
        "\n",
        "# If the values in wei are too large, the softmax becomes very peaky\n",
        "# meaning it assigns almost all the attention to a single token and ignores the rest.\n",
        "# This can hurt learning and make training slower or unstable.\n",
        "# So we scale the scores to normalize them before applying softmax.\n",
        "wei = wei / (head_size ** 0.5) # Scale the scores by sqrt(head_size)\n",
        "\n",
        "# Mask upper triangle (future tokens), to make a decoder\n",
        "# We’re tryyin to decode the sequence so far into the next token,\n",
        "# So we want to prevent the model from looking at future tokens.\n",
        "tri = torch.tril(torch.ones(T, T))\n",
        "wei = wei.masked_fill(tri == 0, float('-inf'))  \n",
        "\n",
        "wei = f.softmax(wei, dim=-1)\n",
        "\n",
        "# Get updated values based on attention scores\n",
        "out = wei @ v  # Shape: (B, T, head_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7ESZ53abwae"
      },
      "source": [
        "# Transformer Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PB-HIhufaLi4"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_steps = 5000\n",
        "eval_iters = 500\n",
        "learning_rate = 1e-3\n",
        "n_embeddings = 32\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embeddings)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embeddings)\n",
        "    self.blocks = nn.Sequential(\n",
        "      TransformerBlock(n_embeddings, n_embeddings // 4),\n",
        "      TransformerBlock(n_embeddings, n_embeddings // 4),\n",
        "      TransformerBlock(n_embeddings, n_embeddings // 4),\n",
        "      TransformerBlock(n_embeddings, n_embeddings // 4),\n",
        "    )\n",
        "\n",
        "    # It maps the output embeddings back to a distribution over the vocabulary\n",
        "    # the term \"head\" refers to the final part of a model that produces the actual output (logits)\n",
        "    self.lm_head = nn.Linear(n_embeddings, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "\n",
        "    tokens_embeddings = self.token_embedding_table(idx) # B, T, n_embeddings\n",
        "    pos_embeddings = self.position_embedding_table(torch.arange(T, device=idx.device)) # T, n_embeddings\n",
        "\n",
        "    #This way, the model understands not just what each word is, but also where it is in the sentence.\n",
        "    x = tokens_embeddings + pos_embeddings # B, T, n_embeddings\n",
        "    x = self.blocks(x)\n",
        "\n",
        "    logits = self.lm_head(x) # B, T, vocab_size\n",
        "\n",
        "    if targets == None:\n",
        "      return logits, None\n",
        "\n",
        "    B, T, C = logits.shape\n",
        "\n",
        "    logits = logits.view(B*T, C)\n",
        "    target = targets.view(B*T)\n",
        "    cost = f.cross_entropy(logits, target)\n",
        "\n",
        "    return logits, cost\n",
        "\n",
        "  ## Idx is B, T\n",
        "  def generate(self, idx, new_tokens):\n",
        "    for _ in range(new_tokens):\n",
        "      logits, _ = self(idx[:, -block_size:])\n",
        "      logits = logits[:, -1, :]\n",
        "\n",
        "      probs = f.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx\n",
        "\n",
        "# Attention: “What should I know from others?”\n",
        "class SingleHeadOfAttention(nn.Module):\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embeddings, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embeddings, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embeddings, head_size, bias=False)\n",
        "    # Registers this matrix as a buffer in the model:\n",
        "    # Include this tensor when saving the model's state with model.state_dict().\n",
        "    # Automatically move it to GPU/CPU along with the model.\n",
        "    self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "  # We recevive the embeddings for each c in t\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape\n",
        "\n",
        "    k = self.key(x) # B, T, head_size\n",
        "    q = self.query(x) # B, T, head_size\n",
        "    v = self.value(x)\n",
        "\n",
        "    wei = q @ k.transpose(-2, -1) * (head_size ** -0.5) # B, T, T\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
        "    wei = f.softmax(wei, dim=-1)\n",
        "\n",
        "    return wei @ v # B, T, head_size\n",
        "\n",
        "# A single attention head can only focus on one type of relationship between tokens at a time.\n",
        "# Multiple heads allow the model to learn multiple perspectives, Syntactic structure, Semantic meaning, Local vs global dependencies\n",
        "class MultiHeadOfAttention(nn.Module):\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([SingleHeadOfAttention(head_size) for _ in range(num_heads)])\n",
        "    # Learns how to combine the outputs of the individual heads in a meaningful wa\n",
        "    self.proj = nn.Linear(n_embeddings, n_embeddings)\n",
        "\n",
        "  def forward(self, x):\n",
        "    res = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    return self.proj(res)\n",
        "\n",
        "# What should I do with that knowledge that I gained from the self attention.\n",
        "# Attention doesn't actually transform the token’s representation in a deep, nonlinear way.\n",
        "# That's the job of the feedforward layer.\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embeddings):\n",
        "    super().__init__()\n",
        "    self.n_embeddings = n_embeddings\n",
        "    self.net = nn.Sequential(\n",
        "      nn.Linear(n_embeddings, n_embeddings * 4), # *4 # Expand (richer representation)\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(n_embeddings * 4, n_embeddings)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "# Allow the model to learn increasingly abstract and complex representations of the input.\n",
        "# First block: each token gathers some context.\n",
        "# Second block: tokens now gather context from already context-aware representations.\n",
        "# Third block: further refined interactions, more abstract patterns.\n",
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, n_embeddings, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embeddings // n_head\n",
        "    self.sa = MultiHeadOfAttention(n_head, head_size)\n",
        "    self.ffwd = FeedForward(n_embeddings)\n",
        "    self.n1 = nn.LayerNorm(n_embeddings)\n",
        "    self.n2 = nn.LayerNorm(n_embeddings)\n",
        "\n",
        "  #Using residual connection to improve vanishing gradient\n",
        "  def forward(self, x):\n",
        "    # Communication\n",
        "    x = x + self.sa(self.n1(x))\n",
        "\n",
        "    # Computation\n",
        "    x = x + self.ffwd(self.n2(x))\n",
        "    return x\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in [\"train\", \"val\"]:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for i in range(eval_iters):\n",
        "      xb, yb = get_batch(split)\n",
        "      logits, cost = model(xb, yb)\n",
        "      losses[i] = cost.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "model = Transformer()\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOSAqk7ZNWuE",
        "outputId": "18de6502-e9ef-4d73-c4e2-ad850214a2dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device cpu\n",
            "batch_size 32\n",
            "step 0: train loss 4.7213, val loss 4.7389\n",
            "step 500: train loss 2.1673, val loss 2.2720\n",
            "step 1000: train loss 1.9730, val loss 2.1379\n",
            "step 1500: train loss 1.8469, val loss 2.0899\n",
            "step 2000: train loss 1.7418, val loss 2.0234\n",
            "step 2500: train loss 1.6732, val loss 2.0309\n",
            "step 3000: train loss 1.5972, val loss 2.0507\n",
            "step 3500: train loss 1.5487, val loss 2.0467\n",
            "step 4000: train loss 1.5082, val loss 2.0734\n",
            "step 4500: train loss 1.4453, val loss 2.0746\n",
            "step 5000: train loss 1.4146, val loss 2.0948\n",
            "Training time: 2.244044085343679\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "start = time.time()\n",
        "print(\"device\", device)\n",
        "print(\"batch_size\", batch_size)\n",
        "\n",
        "for steps in range(max_steps + 1):\n",
        "  xb, yb = get_batch(\"train\")\n",
        "\n",
        "  if steps % eval_iters == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step {steps}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  logits, cost = model(xb, yb)\n",
        "\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "end = time.time()\n",
        "print(\"Training time:\", (end - start) / 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "D5dA1ZKSZLt6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "You have begins inton the will be it's are meent griving up enuty healthy we on assersporationsfition in.\n",
            "The but you putide face brold He nothy Ashonest of days body nes that dish.\n",
            "You have anything.\n",
            "The pace thas to fromple the but dich friends does you leaces to enter the wo hard, the sopphing.\n",
            "You leare on when most prowit.\n",
            "The hese your den the sist dry considered.\n",
            "There just the touly bee resself; to you forces.\n",
            "Don succepter tuth.\n",
            "Geor simpportion nevyor whange obporsest.\n",
            "You do noth is thee something be sensel thing responsibees.\n",
            "Went inimment improsponsh minany of never.\n",
            "You and can happever hespont fore's are of the is the rights.\n",
            "The sknding of the nother.\n",
            "Don't juth taluption.\n",
            "Keech one, enpan you.\n",
            "St insfeed bitoins, braing time profices to spobins to prespotsic torains.\n",
            "Goo have ney to you can heapps afl you brecest of the wo huput everything wise to se byfure.\n",
            "It is work comes preplemprisvent to mannow Bit robice comation the whichs oce finded when trouggets.\n",
            "You will wa\n"
          ]
        }
      ],
      "source": [
        "context = model.generate(torch.zeros(1, 1, dtype=torch.long, device=device), 1000)\n",
        "print(decode(context[0].tolist()))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
